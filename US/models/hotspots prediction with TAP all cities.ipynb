{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62610cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.nn.functional import softmax\n",
    "from torch_geometric.utils import index_to_mask\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.explain import GNNExplainer, Explainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Use CUDA by default\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "def load_dataset(filename: str):\n",
    "    if not Path(f\"../data/TAP-city/{filename}.npz\").is_file():\n",
    "        return None, None\n",
    "    # Load dataset\n",
    "    data_npz = np.load(f\"../data/TAP-city/{filename}.npz\", allow_pickle=True)\n",
    "    \n",
    "    # Extract node features: use only the first two columns\n",
    "    x_raw = data_npz[\"x\"][:, :2]  # assuming 0 = highway (categorical), 1 = street_count\n",
    "    x_raw_tensor = torch.tensor(x_raw, dtype=torch.float)\n",
    "    \n",
    "    # Extract and average edge features\n",
    "    edge_attr_raw = data_npz[\"edge_attr\"][:, :9]  # only first 9 raw edge features\n",
    "    edge_attr_dir = data_npz[\"edge_attr_dir\"]     # (x, y)\n",
    "    edge_attr_ang = data_npz[\"edge_attr_ang\"]     # (min, max, pi)\n",
    "    \n",
    "    # Concatenate raw edge features\n",
    "    full_edge_attr = np.concatenate([edge_attr_raw, edge_attr_dir, edge_attr_ang], axis=1)\n",
    "    \n",
    "    # Map edge features to nodes (average)\n",
    "    edge_index = data_npz[\"edge_index\"]\n",
    "    node_feature_map = defaultdict(list)\n",
    "    \n",
    "    for i, (src, tgt) in enumerate(edge_index):\n",
    "        node_feature_map[src].append(full_edge_attr[i])\n",
    "        node_feature_map[tgt].append(full_edge_attr[i])\n",
    "    \n",
    "    edge_features_per_node = np.array([\n",
    "        np.mean(node_feature_map[i], axis=0) if i in node_feature_map else np.zeros(full_edge_attr.shape[1])\n",
    "        for i in range(x_raw.shape[0])\n",
    "    ])\n",
    "    \n",
    "    # Concatenate node + aggregated edge features\n",
    "    x_final = np.concatenate([x_raw, edge_features_per_node], axis=1)\n",
    "    x_final_tensor = torch.tensor(x_final, dtype=torch.float)\n",
    "    \n",
    "    # Labels and edge_index\n",
    "    y = torch.tensor(data_npz[\"occur_labels\"], dtype=torch.long)\n",
    "    edge_index_tensor = torch.tensor(data_npz[\"edge_index\"].T, dtype=torch.long)\n",
    "    \n",
    "    # Final PyG graph\n",
    "    data = Data(x=x_final_tensor, y=y, edge_index=edge_index_tensor)\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    all_idx = torch.arange(data.num_nodes)\n",
    "    train_idx, test_idx = train_test_split(all_idx, test_size=0.2, stratify=y, random_state=42)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=0.25, stratify=y[train_idx], random_state=42)\n",
    "    \n",
    "    data.train_mask = index_to_mask(train_idx, size=data.num_nodes)\n",
    "    data.val_mask = index_to_mask(val_idx, size=data.num_nodes)\n",
    "    data.test_mask = index_to_mask(test_idx, size=data.num_nodes)\n",
    "    return data, data_npz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd5d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def fetch_cities() -> dict[str, str]:\n",
    "    cities = {}\n",
    "    for file in os.listdir(\"../data/TAP-city\"):\n",
    "        cities[file.split(\".\")[0]] = file.split(\".\")[0]\n",
    "    return cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202cd9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def setup(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self(data.x, data.edge_index)\n",
    "            pred = out[data.val_mask].argmax(dim=1)\n",
    "            correct = (pred == data.y[data.val_mask]).sum().item()\n",
    "            acc = correct / data.val_mask.sum().item()\n",
    "        return acc\n",
    "    \n",
    "    def train_model(self, data):\n",
    "        self.train()\n",
    "        self.loss_list = []\n",
    "        self.val_acc_list = []\n",
    "        for epoch in range(200):\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self(data.x, data.edge_index)\n",
    "            loss = self.loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            val_acc = self.evaluate(data)\n",
    "\n",
    "            # Store values for plotting\n",
    "            self.loss_list.append(loss.item())\n",
    "            self.val_acc_list.append(val_acc)\n",
    "\n",
    "            # Print every 10 epochs\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    def plot_loss_and_accuracy(self, data_name):\n",
    "        if not self.loss_list or not self.val_acc_list:\n",
    "            print(\"No loss computed yet\")\n",
    "            return\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.loss_list, label=\"Train Loss\", color='blue')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Over Epochs\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_acc_list, label=\"Validation Accuracy\", color='green')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Validation Accuracy Over Epochs\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Create a folder for data_name\n",
    "        os.makedirs(f\"../output/{data_name}\", exist_ok=True)\n",
    "        plt.savefig(f\"../output/{data_name}/loss_and_accuracy.png\")        \n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self(data.x, data.edge_index)\n",
    "            probs = softmax(out, dim=1)\n",
    "            pred = probs.argmax(dim=1)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3cd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT model\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def setup(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self(data.x, data.edge_index)\n",
    "            pred = out[data.val_mask].argmax(dim=1)\n",
    "            correct = (pred == data.y[data.val_mask]).sum().item()\n",
    "            acc = correct / data.val_mask.sum().item()\n",
    "        return acc\n",
    "    \n",
    "    def train_model(self, data):\n",
    "        self.train()\n",
    "        self.loss_list = []\n",
    "        self.val_acc_list = []\n",
    "        for epoch in range(200):\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self(data.x, data.edge_index)\n",
    "            loss = self.loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            val_acc = self.evaluate(data)\n",
    "\n",
    "            # Store values for plotting\n",
    "            self.loss_list.append(loss.item())\n",
    "            self.val_acc_list.append(val_acc)\n",
    "\n",
    "            # Print every 10 epochs\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    def plot_loss_and_accuracy(self, data_name):\n",
    "        if not self.loss_list or not self.val_acc_list:\n",
    "            print(\"No loss computed yet\")\n",
    "            return\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.loss_list, label=\"Train Loss\", color='blue')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Over Epochs\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_acc_list, label=\"Validation Accuracy\", color='green')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Validation Accuracy Over Epochs\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Create a folder for data_name\n",
    "        os.makedirs(f\"../output/{data_name}\", exist_ok=True)\n",
    "        plt.savefig(f\"../output/{data_name}/loss_and_accuracy.png\")        \n",
    "        \n",
    "\n",
    "    def predict(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self(data.x, data.edge_index)\n",
    "            probs = softmax(out, dim=1)\n",
    "            pred = probs.argmax(dim=1)\n",
    "            return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d26116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(data_npz, data, model, preds):\n",
    "    # Load coordinates\n",
    "    coordinates = data_npz[\"coordinates\"]\n",
    "    print(len(coordinates))\n",
    "    assert len(coordinates) == data.x.shape[0], \"Mismatch in coordinate and node count.\"\n",
    "\n",
    "    feature_names = [\n",
    "        'node_highway', 'street_count',\n",
    "        'edge_highway', 'edge_length', 'edge_bridge', 'edge_lanes', 'edge_oneway',\n",
    "        'edge_maxspeed', 'edge_access', 'edge_tunnel', 'edge_junction',\n",
    "        'edge_dir_x', 'edge_dir_y',\n",
    "        'angle_min', 'angle_max', 'angle_pi'\n",
    "    ]\n",
    "\n",
    "    # Padding if your final feature count exceeds 14\n",
    "    if data.x.shape[1] > len(feature_names):\n",
    "        feature_names += [f\"feature_{i}\" for i in range(len(feature_names), data.x.shape[1])]\n",
    "\n",
    "    # GNNExplainer\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=GNNExplainer(epochs=30),\n",
    "        explanation_type='model',\n",
    "        node_mask_type='attributes',\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='multiclass_classification',\n",
    "            task_level='node',\n",
    "            return_type='log_probs',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # For each predicted hotspot, explain and output top features\n",
    "    k = 5  # top-k features to show\n",
    "    print(f\"\\n🔍 HOTSPOT ANALYSIS (Top {k} Features Per Hotspot Node):\\n\")\n",
    "    for node_idx in range(data.num_nodes):\n",
    "        if preds[node_idx] == 1:  # predicted hotspot\n",
    "            explanation = explainer(data.x, data.edge_index, index=int(node_idx))\n",
    "            feat_mask = explanation.node_mask[node_idx].cpu().detach().numpy()\n",
    "\n",
    "            # Rank top-k features\n",
    "            top_k = np.argsort(-feat_mask)[:k]\n",
    "            feature_scores = [(feature_names[i], feat_mask[i]) for i in top_k]\n",
    "\n",
    "            # Output\n",
    "            lat, lon = coordinates[node_idx]\n",
    "            print(f\"📍 Node {node_idx} | Coordinates: ({lat:.6f}, {lon:.6f})\")\n",
    "            for name, score in feature_scores:\n",
    "                print(f\"   - {name}: {score:.4f}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54bb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "cities = fetch_cities()\n",
    "\n",
    "outputs = {}\n",
    "for city_name, filename in cities.items():\n",
    "    # If city already exists, skip\n",
    "    filepath = Path(f\"../output/{city_name}/\")\n",
    "    if filepath.is_dir():\n",
    "        # print(f\"{city_name} folder exists\")\n",
    "        continue\n",
    "\n",
    "    data, data_npz = load_dataset(filename)\n",
    "    if data is None:\n",
    "        continue\n",
    "    gcn = GCN(in_channels=data.x.shape[1], hidden_channels=16, out_channels=2)\n",
    "    gcn.setup()\n",
    "    gcn.train_model(data)\n",
    "    gcn.plot_loss_and_accuracy(city_name)\n",
    "    preds = gcn.predict(data)\n",
    "    outputs[city_name] = preds\n",
    "    # explain(data_npz, data, gcn, preds)\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(f\"../output/{city_name}\", exist_ok=True)\n",
    "    torch.save(gcn.state_dict(), f\"../output/{city_name}/model.pth\")\n",
    "    # Save the outputs\n",
    "    with open(f\"../output/{city_name}/outputs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(outputs[city_name], f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f9eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_cities = [\n",
    "  \"new_york_ny\",\n",
    "  \"oklahoma_city_ok\", \n",
    "  \"los_angeles_ca\",\n",
    "  \"houston_tx\",\n",
    "  \"new_york_ny\",\n",
    "  \"phoenix_az\",\n",
    "  \"san_antonio_tx\",\n",
    "  \"dallas_tx\",\n",
    "  \"jacksonville_fl\",\n",
    "  \"indianapolis_in\",\n",
    "  \"charlotte_nc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Not implemented yet\n",
    "\n",
    "#Top recurring features in hotspots\n",
    "k = 5\n",
    "feature_counter = Counter()\n",
    "\n",
    "for node_idx in range(data.num_nodes):\n",
    "    if preds[node_idx] == 1:  # Only for predicted hotspots\n",
    "        explanation = explainer(data.x, data.edge_index, index=int(node_idx))\n",
    "        feat_mask = explanation.node_mask[node_idx].cpu().detach().numpy()\n",
    "\n",
    "        # Get top-k important feature indices\n",
    "        top_k_indices = np.argsort(-feat_mask)[:k]\n",
    "\n",
    "        # Add feature names to counter\n",
    "        feature_counter.update([feature_names[i] for i in top_k_indices])\n",
    "\n",
    "# Sort and display most frequent top features\n",
    "most_common_features = feature_counter.most_common(10)\n",
    "print(\"\\nMost Common Top Features for Hotspot Predictions:\")\n",
    "for name, freq in most_common_features:\n",
    "    print(f\"{name}: {freq} times in top-{k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba97ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotspot_heatmap(coordinates, preds, city_name):\n",
    "  # Don't generate if heatmap already exists\n",
    "  if Path(f\"../output/{city_name}/hotspot_heatmap.png\").is_file():\n",
    "    return\n",
    "  #Hotspot heatmap\n",
    "  hotspot_coords = coordinates[preds == 1]\n",
    "  non_hotspot_coords = coordinates[preds == 0]\n",
    "\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plt.scatter(non_hotspot_coords[:, 1], non_hotspot_coords[:, 0],\n",
    "              c='skyblue', s=10, label='Non-Hotspots', alpha=0.5)\n",
    "  plt.scatter(hotspot_coords[:, 1], hotspot_coords[:, 0],\n",
    "              c='red', s=20, label='Hotspots', alpha=0.8)\n",
    "\n",
    "  plt.xlabel(\"Longitude\")\n",
    "  plt.ylabel(\"Latitude\")\n",
    "  plt.title(\"Hotspot Prediction Map\")\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"../output/{city_name}/hotspot_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6dd7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hotspot_geojson(coordinates, preds, city_name):\n",
    "    # Generate geojson\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "    for i, (lat, lon) in enumerate(coordinates):\n",
    "        if preds[i] == 1:\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [lon, lat]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"name\": f\"{city_name} hotspot {i}\"\n",
    "                }\n",
    "            }\n",
    "            geojson[\"features\"].append(feature)\n",
    "    return geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501986c3-9e66-4e19-bf9a-4a65195ac581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating geojson for providence_ri: 0\n",
      "Error generating geojson for provo_ut: 0\n",
      "Error generating geojson for pueblo_co: 0\n",
      "Error generating geojson for punta_gorda_fl: 0\n",
      "Error generating geojson for puyallup_wa: 0\n",
      "Error generating geojson for quincy_fl: 0\n",
      "Error generating geojson for rainbow_cdp_ca: 0\n",
      "Error generating geojson for rainier_or: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cities = fetch_cities()\n",
    "\n",
    "# Load the model and outputs\n",
    "for city_name in cities.keys():\n",
    "    if not Path(f\"../output/{city_name}\").is_dir():\n",
    "        continue\n",
    "    # If city geojson already exists, skip\n",
    "    if Path(f\"../output/{city_name}/hotspot_geojson.json\").is_file():\n",
    "        continue\n",
    "\n",
    "    # print(f\"Fetching heatmap for {city_name}\")\n",
    "    data, data_npz = load_dataset(city_name)\n",
    "\n",
    "    model = GCN(in_channels=data.x.shape[1], hidden_channels=16, out_channels=2)\n",
    "    model.load_state_dict(torch.load(f\"../output/{city_name}/model.pth\"))\n",
    "\n",
    "    with open(f\"../output/{city_name}/outputs.pkl\", \"rb\") as file:\n",
    "        outputs = pickle.load(file)\n",
    "        hotspot_heatmap(data_npz[\"coordinates\"], outputs, city_name)\n",
    "        try:\n",
    "            geojson = generate_hotspot_geojson(data_npz[\"coordinates\"], outputs, city_name)\n",
    "            # If file exists, delete it\n",
    "            if Path(f\"../output/{city_name}/hotspot_geojson.json\").is_file():\n",
    "                os.remove(f\"../output/{city_name}/hotspot_geojson.json\")\n",
    "            with open(f\"../output/{city_name}/hotspot_geojson.json\", \"w\") as f:\n",
    "                json.dump(geojson, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating geojson for {city_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec06cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a geojson file with the top 10 cities\n",
    "top_10_cities_geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "for city_name in top_10_cities:\n",
    "    with open(f\"../output/{city_name}/hotspot_geojson.json\", \"r\") as f:\n",
    "        geojson = json.load(f)\n",
    "        top_10_cities_geojson[\"features\"].extend(geojson[\"features\"])\n",
    "\n",
    "with open(\"../output/top_10_cities_geojson.json\", \"w\") as f:\n",
    "    json.dump(top_10_cities_geojson, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da372194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the top 10 cities geojson file, create a map with folium\n",
    "import folium\n",
    "\n",
    "# Create a map centered on the first city's coordinates\n",
    "map = folium.Map(location=[top_10_cities_geojson[\"features\"][0][\"geometry\"][\"coordinates\"][1], top_10_cities_geojson[\"features\"][0][\"geometry\"][\"coordinates\"][0]], zoom_start=12)\n",
    "\n",
    "# Add the top 10 cities geojson to the map\n",
    "folium.GeoJson(top_10_cities_geojson).add_to(map)\n",
    "\n",
    "# Save the map to an html file\n",
    "map.save(\"../output/top_10_cities_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
