{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Road Traffic Accident Data (Jan 2009 - Feb 2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accident Dataset:\n",
      "                            DataSeries  2025Feb  2025Jan  2024Dec  2024Nov  \\\n",
      "0          Total Casualties Fatalities        9       12       16       15   \n",
      "1                          Pedestrians        1        4        2        2   \n",
      "2       Personal Mobility Device Users        0        0        0        0   \n",
      "3                  Cyclists & Pillions        3        2        2        0   \n",
      "4      Motor Cyclists & Pillion Riders        4        6        9       10   \n",
      "\n",
      "   2024Oct  2024Sep  2024Aug  2024Jul  2024Jun  ...  2009Oct  2009Sep  \\\n",
      "0       10       10        8       11       13  ...       13       18   \n",
      "1        1        1        2        3        2  ...        3        4   \n",
      "2        0        0        0        0        0  ...       na       na   \n",
      "3        1        1        1        1        1  ...        1        3   \n",
      "4        7        5        4        6        9  ...        7        9   \n",
      "\n",
      "   2009Aug  2009Jul  2009Jun  2009May  2009Apr  2009Mar  2009Feb  2009Jan  \n",
      "0       14       16       10       22       14       11       12       13  \n",
      "1        3        4        5        2        3        3        3        4  \n",
      "2       na       na       na       na       na       na       na       na  \n",
      "3        1        2        1        2        3        1        0        0  \n",
      "4       10        8        4        8        4        6        8        7  \n",
      "\n",
      "[5 rows x 195 columns]\n"
     ]
    }
   ],
   "source": [
    "#Road Traffic Accident Data\n",
    "accident_df = pd.read_csv(\"data/RoadTrafficAccidentCasualtiesMonthly.csv\")\n",
    "print(\"Accident Dataset:\")\n",
    "print(accident_df.head())  # Display first few rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather Data  - Rainfall across SG (Dec 2016 to Mar 2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rainfall JSON Structure:\n",
      "{\n",
      "  \"openapi\": \"3.0.3\",\n",
      "  \"info\": {\n",
      "    \"title\": \"Real-time API weather services\",\n",
      "    \"description\": \"Real-time API documentation of weather services\",\n",
      "    \"contact\": {\n",
      "      \"email\": \"feedback@data.gov.sg\"\n",
      "    },\n",
      "    \"version\": \"1.0.11\"\n",
      "  },\n",
      "  \"servers\": [\n",
      "    {\n",
      "      \"url\": \"https://api-open.data.gov.sg/v2/real-time/api\"\n",
      "    }\n",
      "  ],\n",
      "  \"paths\": {\n",
      "    \"/rainfall\": {\n",
      "      \"get\": {\n",
      "        \"summary\": \"Get rainfall readings across Singapore\",\n",
      "        \"description\": \"**[https://api-open.data.gov.sg/v2/real-time/api/rainfall](https://api-open.data.gov.sg/v2/real-time/api/rainfall)**\\n\\n<br/>\\n\\n- 5-minute readings from NEA\\n\\n<br/>\\n\\n- Filter for specific date or date-time by providing `date` in query parameter.\\n  - use YYYY-MM-DD format to retrieve all of the readings for that day\\n  - use YYYY-MM-DDTHH:mm:ss to retrieve the latest readings at that moment in time\\n  - example: `?date=2024-07-16` or `?date=2024-07-16T23:59:00`\\n\",\n",
      "        \"parameters\": [\n",
      "          {\n",
      "            \"in\": \"query\",\n",
      "            \"name\": \"date\",\n",
      "            \"description\": \"Format: YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS (SGT). Example: 2024-07-16 or 2024-07-16T23:59:00\",\n",
      "            \"schema\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"in\": \"query\",\n",
      "            \"name\": \"paginationToken\",\n",
      "            \"description\": \"Pagination token for retrieving subsequent data pages\",\n",
      "            \"schema\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"responses\": {\n",
      "          \"200\": {\n",
      "            \"description\": \"Rainfall Information\",\n",
      "            \"content\": {\n",
      "              \"application/json\": {\n",
      "                \"schema\": {\n",
      "                  \"type\": \"object\",\n",
      "                  \"properties\": {\n",
      "                    \"code\": {\n",
      "                      \"type\": \"integer\",\n",
      "                      \"description\": \"Response status code (always 0 for success)\"\n",
      "                    },\n",
      "                    \"errorMsg\": {\n",
      "                      \"type\": \"string\",\n",
      "                      \"description\": \"Error message (empty string for success)\",\n",
      "                      \"example\": null\n",
      "                    },\n",
      "                    \"data\": {\n",
      "                      \"type\": \"object\",\n",
      "                      \"properties\": {\n",
      "                        \"stations\": {\n",
      "                          \"type\": \"array\",\n",
      "                          \"items\": {\n",
      "                            \"type\": \"object\",\n",
      "                            \"properties\": {\n",
      "                              \"id\": {\n",
      "                                \"type\": \"string\",\n",
      "                                \"description\": \"Station's ID\",\n",
      "                                \"example\": \"S111\"\n",
      "                              },\n",
      "                              \"deviceId\": {\n",
      "                                \"type\": \"string\",\n",
      "                                \"description\": \"Reading Device's ID (usually same as Station's ID)\",\n",
      "                                \"example\": \"S111\"\n",
      "                              },\n",
      "                              \"name\": {\n",
      "                                \"type\": \"string\",\n",
      "                                \"description\": \"Station's name\",\n",
      "                                \"example\": \"Scotts Road\"\n",
      "                              },\n",
      "                              \"labelLocation\": {\n",
      "                                \"type\": \"object\",\n",
      "                                \"properties\": {\n",
      "                                  \"latitude\": {\n",
      "                                    \"type\": \"number\",\n",
      "                                    \"description\": \"Latitude coordinate of the region label\",\n",
      "                                    \"example\": 1.31055\n",
      "                                  },\n",
      "                                  \"longitude\": {\n",
      "                                    \"type\": \"number\",\n",
      "                                    \"description\": \"Longitude coordinate of the region label\",\n",
      "                                    \"example\": 103.8365\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"readings\": {\n",
      "                          \"type\": \"array\",\n",
      "                          \"items\": {\n",
      "                            \"type\": \"object\",\n",
      "                            \"properties\": {\n",
      "                              \"timestamp\": {\n",
      "                                \"type\": \"string\",\n",
      "                                \"example\": \"2024-07-17T14:00:00.000Z\"\n",
      "                              },\n",
      "                              \"data\": {\n",
      "                                \"type\": \"array\",\n",
      "                                \"items\": {\n",
      "                                  \"type\": \"object\",\n",
      "                                  \"properties\": {\n",
      "                                    \"stationId\": {\n",
      "                                      \"type\": \"string\",\n",
      "                                      \"example\": \"S111\"\n",
      "                                    },\n",
      "                                    \"value\": {\n",
      "                                      \"type\": \"number\",\n",
      "                                      \"example\": 2\n",
      "                                    }\n",
      "                                  }\n",
      "                                }\n",
      "                              }\n",
      "                            }\n",
      "                          }\n",
      "                        },\n",
      "                        \"readingType\": {\n",
      "                          \"type\": \"string\",\n",
      "                          \"description\": \"Information about the reading\",\n",
      "                          \"example\": \"TB1 Rainfall 5 Minute Total F\"\n",
      "                        },\n",
      "                        \"readingUnit\": {\n",
      "                          \"type\": \"string\",\n",
      "                          \"description\": \"Measurement unit for reading\",\n",
      "                          \"example\": \"mm\"\n",
      "                        },\n",
      "                        \"paginationToken\": {\n",
      "                          \"type\": \"string\",\n",
      "                          \"description\": \"Token to retrieve next page if exists\",\n",
      "                          \"example\": \"b2Zmc2V0PTEwMA== (you will see this token only if next page exists)\"\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          \"400\": {\n",
      "            \"description\": \"Invalid HTTP request body\",\n",
      "            \"content\": {\n",
      "              \"application/json\": {\n",
      "                \"schema\": {\n",
      "                  \"type\": \"object\",\n",
      "                  \"properties\": {\n",
      "                    \"code\": {\n",
      "                      \"type\": \"number\",\n",
      "                      \"example\": 4\n",
      "                    },\n",
      "                    \"name\": {\n",
      "                      \"type\": \"string\",\n",
      "                      \"example\": \"ERROR_PARAMS\"\n",
      "                    },\n",
      "                    \"data\": {\n",
      "                      \"type\": \"object\",\n",
      "                      \"example\": null\n",
      "                    },\n",
      "                    \"errorMsg\": {\n",
      "                      \"type\": \"string\",\n",
      "                      \"enum\": [\n",
      "                        \"Invalid date format. Date format must be YYYY-MM-DD (2024-06-01) or YYYY-MM-DDTHH:mm:ss (2024-06-01T08:30:00).\",\n",
      "                        \"Invalid pagination token.\"\n",
      "                      ]\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          \"404\": {\n",
      "            \"description\": \"Weather data not found\",\n",
      "            \"content\": {\n",
      "              \"application/json\": {\n",
      "                \"schema\": {\n",
      "                  \"type\": \"object\",\n",
      "                  \"properties\": {\n",
      "                    \"code\": {\n",
      "                      \"type\": \"number\",\n",
      "                      \"example\": 17\n",
      "                    },\n",
      "                    \"name\": {\n",
      "                      \"type\": \"string\",\n",
      "                      \"example\": \"REAL_TIME_API_DATA_NOT_FOUND\"\n",
      "                    },\n",
      "                    \"data\": {\n",
      "                      \"type\": \"object\",\n",
      "                      \"example\": null\n",
      "                    },\n",
      "                    \"errorMsg\": {\n",
      "                      \"type\": \"string\",\n",
      "                      \"example\": \"Data not found\"\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Rainfall across SG \n",
    "with open(\"RainfallacrossSingapore.json\", \"r\") as file:\n",
    "    rainfall_data = json.load(file)\n",
    "\n",
    "print(\"\\nRainfall JSON Structure:\")\n",
    "print(json.dumps(rainfall_data, indent=2))  # Pretty print JSON structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2024-01-01...\n",
      "Fetching data for 2024-01-02...\n",
      "Fetching data for 2024-01-03...\n",
      "Fetching data for 2024-01-04...\n",
      "Fetching data for 2024-01-05...\n",
      "Fetching data for 2024-01-06...\n",
      "Fetching data for 2024-01-07...\n",
      "Fetching data for 2024-01-08...\n",
      "Fetching data for 2024-01-09...\n",
      "Fetching data for 2024-01-10...\n",
      "Fetching data for 2024-01-11...\n",
      "Fetching data for 2024-01-12...\n",
      "Fetching data for 2024-01-13...\n",
      "Fetching data for 2024-01-14...\n",
      "Fetching data for 2024-01-15...\n",
      "Fetching data for 2024-01-16...\n",
      "Fetching data for 2024-01-17...\n",
      "Fetching data for 2024-01-18...\n",
      "Fetching data for 2024-01-19...\n",
      "Fetching data for 2024-01-20...\n",
      "Fetching data for 2024-01-21...\n",
      "Fetching data for 2024-01-22...\n",
      "Fetching data for 2024-01-23...\n",
      "Fetching data for 2024-01-24...\n",
      "Fetching data for 2024-01-25...\n",
      "Fetching data for 2024-01-26...\n",
      "Fetching data for 2024-01-27...\n",
      "Fetching data for 2024-01-28...\n",
      "Fetching data for 2024-01-29...\n",
      "Fetching data for 2024-01-30...\n",
      "Fetching data for 2024-01-31...\n",
      "Fetching data for 2024-02-01...\n",
      "Fetching data for 2024-02-02...\n",
      "Fetching data for 2024-02-03...\n",
      "Fetching data for 2024-02-04...\n",
      "Fetching data for 2024-02-05...\n",
      "Fetching data for 2024-02-06...\n",
      "Fetching data for 2024-02-07...\n",
      "Fetching data for 2024-02-08...\n",
      "Fetching data for 2024-02-09...\n",
      "Fetching data for 2024-02-10...\n",
      "Fetching data for 2024-02-11...\n",
      "Fetching data for 2024-02-12...\n",
      "Fetching data for 2024-02-13...\n",
      "Fetching data for 2024-02-14...\n",
      "Fetching data for 2024-02-15...\n",
      "Fetching data for 2024-02-16...\n",
      "Fetching data for 2024-02-17...\n",
      "Fetching data for 2024-02-18...\n",
      "Fetching data for 2024-02-19...\n",
      "Fetching data for 2024-02-20...\n",
      "Fetching data for 2024-02-21...\n",
      "Fetching data for 2024-02-22...\n",
      "Fetching data for 2024-02-23...\n",
      "Fetching data for 2024-02-24...\n",
      "Fetching data for 2024-02-25...\n",
      "Fetching data for 2024-02-26...\n",
      "Fetching data for 2024-02-27...\n",
      "Fetching data for 2024-02-28...\n",
      "Fetching data for 2024-02-29...\n",
      "Fetching data for 2024-03-01...\n",
      "Fetching data for 2024-03-02...\n",
      "Fetching data for 2024-03-03...\n",
      "Fetching data for 2024-03-04...\n",
      "Fetching data for 2024-03-05...\n",
      "Fetching data for 2024-03-06...\n",
      "Fetching data for 2024-03-07...\n",
      "Fetching data for 2024-03-08...\n",
      "Fetching data for 2024-03-09...\n",
      "Fetching data for 2024-03-10...\n",
      "Fetching data for 2024-03-11...\n",
      "Fetching data for 2024-03-12...\n",
      "Fetching data for 2024-03-13...\n",
      "Fetching data for 2024-03-14...\n",
      "Fetching data for 2024-03-15...\n",
      "Fetching data for 2024-03-16...\n",
      "Fetching data for 2024-03-17...\n",
      "Fetching data for 2024-03-18...\n",
      "Fetching data for 2024-03-19...\n",
      "Fetching data for 2024-03-20...\n",
      "Fetching data for 2024-03-21...\n",
      "Fetching data for 2024-03-22...\n",
      "Fetching data for 2024-03-23...\n",
      "Fetching data for 2024-03-24...\n",
      "Fetching data for 2024-03-25...\n",
      "Fetching data for 2024-03-26...\n",
      "Fetching data for 2024-03-27...\n",
      "Fetching data for 2024-03-28...\n",
      "Fetching data for 2024-03-29...\n",
      "Fetching data for 2024-03-30...\n",
      "Fetching data for 2024-03-31...\n",
      "Fetching data for 2024-04-01...\n",
      "Fetching data for 2024-04-02...\n",
      "Fetching data for 2024-04-03...\n",
      "Fetching data for 2024-04-04...\n",
      "Fetching data for 2024-04-05...\n",
      "Fetching data for 2024-04-06...\n",
      "Fetching data for 2024-04-07...\n",
      "Fetching data for 2024-04-08...\n",
      "Fetching data for 2024-04-09...\n",
      "Fetching data for 2024-04-10...\n",
      "Fetching data for 2024-04-11...\n",
      "Fetching data for 2024-04-12...\n",
      "Fetching data for 2024-04-13...\n",
      "Fetching data for 2024-04-14...\n",
      "Fetching data for 2024-04-15...\n",
      "Fetching data for 2024-04-16...\n",
      "Fetching data for 2024-04-17...\n",
      "Fetching data for 2024-04-18...\n",
      "Fetching data for 2024-04-19...\n",
      "Fetching data for 2024-04-20...\n",
      "Fetching data for 2024-04-21...\n",
      "Fetching data for 2024-04-22...\n",
      "Fetching data for 2024-04-23...\n",
      "Fetching data for 2024-04-24...\n",
      "Fetching data for 2024-04-25...\n",
      "Fetching data for 2024-04-26...\n",
      "Fetching data for 2024-04-27...\n",
      "Fetching data for 2024-04-28...\n",
      "Fetching data for 2024-04-29...\n",
      "Fetching data for 2024-04-30...\n",
      "Fetching data for 2024-05-01...\n",
      "Fetching data for 2024-05-02...\n",
      "Fetching data for 2024-05-03...\n",
      "Fetching data for 2024-05-04...\n",
      "Fetching data for 2024-05-05...\n",
      "Fetching data for 2024-05-06...\n",
      "Fetching data for 2024-05-07...\n",
      "Fetching data for 2024-05-08...\n",
      "Fetching data for 2024-05-09...\n",
      "Fetching data for 2024-05-10...\n",
      "Fetching data for 2024-05-11...\n",
      "Fetching data for 2024-05-12...\n",
      "Fetching data for 2024-05-13...\n",
      "Fetching data for 2024-05-14...\n",
      "Fetching data for 2024-05-15...\n",
      "Fetching data for 2024-05-16...\n",
      "Fetching data for 2024-05-17...\n",
      "Fetching data for 2024-05-18...\n",
      "Fetching data for 2024-05-19...\n",
      "Fetching data for 2024-05-20...\n",
      "Fetching data for 2024-05-21...\n",
      "Fetching data for 2024-05-22...\n",
      "Fetching data for 2024-05-23...\n",
      "Fetching data for 2024-05-24...\n",
      "Fetching data for 2024-05-25...\n",
      "Fetching data for 2024-05-26...\n",
      "Fetching data for 2024-05-27...\n",
      "Fetching data for 2024-05-28...\n",
      "Fetching data for 2024-05-29...\n",
      "Fetching data for 2024-05-30...\n",
      "Fetching data for 2024-05-31...\n",
      "Fetching data for 2024-06-01...\n",
      "Fetching data for 2024-06-02...\n",
      "Fetching data for 2024-06-03...\n",
      "Fetching data for 2024-06-04...\n",
      "Fetching data for 2024-06-05...\n",
      "Fetching data for 2024-06-06...\n",
      "Fetching data for 2024-06-07...\n",
      "Fetching data for 2024-06-08...\n",
      "Fetching data for 2024-06-09...\n",
      "Fetching data for 2024-06-10...\n",
      "Fetching data for 2024-06-11...\n",
      "Fetching data for 2024-06-12...\n",
      "Fetching data for 2024-06-13...\n",
      "Fetching data for 2024-06-14...\n",
      "Fetching data for 2024-06-15...\n",
      "Fetching data for 2024-06-16...\n",
      "Fetching data for 2024-06-17...\n",
      "Fetching data for 2024-06-18...\n",
      "Fetching data for 2024-06-19...\n",
      "Fetching data for 2024-06-20...\n",
      "Fetching data for 2024-06-21...\n",
      "Fetching data for 2024-06-22...\n",
      "Fetching data for 2024-06-23...\n",
      "Fetching data for 2024-06-24...\n",
      "Fetching data for 2024-06-25...\n",
      "Fetching data for 2024-06-26...\n",
      "Fetching data for 2024-06-27...\n",
      "Fetching data for 2024-06-28...\n",
      "Fetching data for 2024-06-29...\n",
      "Fetching data for 2024-06-30...\n",
      "Fetching data for 2024-07-01...\n",
      "Fetching data for 2024-07-02...\n",
      "Fetching data for 2024-07-03...\n",
      "Fetching data for 2024-07-04...\n",
      "Fetching data for 2024-07-05...\n",
      "Fetching data for 2024-07-06...\n",
      "Fetching data for 2024-07-07...\n",
      "Fetching data for 2024-07-08...\n",
      "Fetching data for 2024-07-09...\n",
      "Fetching data for 2024-07-10...\n",
      "Fetching data for 2024-07-11...\n",
      "Fetching data for 2024-07-12...\n",
      "Fetching data for 2024-07-13...\n",
      "Fetching data for 2024-07-14...\n",
      "Fetching data for 2024-07-15...\n",
      "Fetching data for 2024-07-16...\n",
      "Fetching data for 2024-07-17...\n",
      "Fetching data for 2024-07-18...\n",
      "Fetching data for 2024-07-19...\n",
      "Fetching data for 2024-07-20...\n",
      "Fetching data for 2024-07-21...\n",
      "Fetching data for 2024-07-22...\n",
      "Fetching data for 2024-07-23...\n",
      "Fetching data for 2024-07-24...\n",
      "Fetching data for 2024-07-25...\n",
      "Fetching data for 2024-07-26...\n",
      "Fetching data for 2024-07-27...\n",
      "Fetching data for 2024-07-28...\n",
      "Fetching data for 2024-07-29...\n",
      "Fetching data for 2024-07-30...\n",
      "Fetching data for 2024-07-31...\n",
      "Fetching data for 2024-08-01...\n",
      "Fetching data for 2024-08-02...\n",
      "Fetching data for 2024-08-03...\n",
      "Fetching data for 2024-08-04...\n",
      "Fetching data for 2024-08-05...\n",
      "Fetching data for 2024-08-06...\n",
      "Fetching data for 2024-08-07...\n",
      "Fetching data for 2024-08-08...\n",
      "Fetching data for 2024-08-09...\n",
      "Fetching data for 2024-08-10...\n",
      "Fetching data for 2024-08-11...\n",
      "Fetching data for 2024-08-12...\n",
      "Fetching data for 2024-08-13...\n",
      "Fetching data for 2024-08-14...\n",
      "Fetching data for 2024-08-15...\n",
      "Fetching data for 2024-08-16...\n",
      "Fetching data for 2024-08-17...\n",
      "Fetching data for 2024-08-18...\n",
      "Fetching data for 2024-08-19...\n",
      "Fetching data for 2024-08-20...\n",
      "Fetching data for 2024-08-21...\n",
      "Fetching data for 2024-08-22...\n",
      "Fetching data for 2024-08-23...\n",
      "Fetching data for 2024-08-24...\n",
      "Fetching data for 2024-08-25...\n",
      "Fetching data for 2024-08-26...\n",
      "Fetching data for 2024-08-27...\n",
      "Fetching data for 2024-08-28...\n",
      "Fetching data for 2024-08-29...\n",
      "Fetching data for 2024-08-30...\n",
      "Fetching data for 2024-08-31...\n",
      "Fetching data for 2024-09-01...\n",
      "Fetching data for 2024-09-02...\n",
      "Fetching data for 2024-09-03...\n",
      "Fetching data for 2024-09-04...\n",
      "Fetching data for 2024-09-05...\n",
      "Fetching data for 2024-09-06...\n",
      "Fetching data for 2024-09-07...\n",
      "Fetching data for 2024-09-08...\n",
      "Fetching data for 2024-09-09...\n",
      "Fetching data for 2024-09-10...\n",
      "Fetching data for 2024-09-11...\n",
      "Fetching data for 2024-09-12...\n",
      "Fetching data for 2024-09-13...\n",
      "Fetching data for 2024-09-14...\n",
      "Fetching data for 2024-09-15...\n",
      "Fetching data for 2024-09-16...\n",
      "Fetching data for 2024-09-17...\n",
      "Fetching data for 2024-09-18...\n",
      "Fetching data for 2024-09-19...\n",
      "Fetching data for 2024-09-20...\n",
      "Fetching data for 2024-09-21...\n",
      "Fetching data for 2024-09-22...\n",
      "Fetching data for 2024-09-23...\n",
      "Fetching data for 2024-09-24...\n",
      "Fetching data for 2024-09-25...\n",
      "Fetching data for 2024-09-26...\n",
      "Fetching data for 2024-09-27...\n",
      "Fetching data for 2024-09-28...\n",
      "Fetching data for 2024-09-29...\n",
      "Fetching data for 2024-09-30...\n",
      "Fetching data for 2024-10-01...\n",
      "Fetching data for 2024-10-02...\n",
      "Fetching data for 2024-10-03...\n",
      "Fetching data for 2024-10-04...\n",
      "Fetching data for 2024-10-05...\n",
      "Fetching data for 2024-10-06...\n",
      "Fetching data for 2024-10-07...\n",
      "Fetching data for 2024-10-08...\n",
      "Fetching data for 2024-10-09...\n",
      "Fetching data for 2024-10-10...\n",
      "Fetching data for 2024-10-11...\n",
      "Fetching data for 2024-10-12...\n",
      "Fetching data for 2024-10-13...\n",
      "Fetching data for 2024-10-14...\n",
      "Fetching data for 2024-10-15...\n",
      "Fetching data for 2024-10-16...\n",
      "Fetching data for 2024-10-17...\n",
      "Fetching data for 2024-10-18...\n",
      "Fetching data for 2024-10-19...\n",
      "Fetching data for 2024-10-20...\n",
      "Fetching data for 2024-10-21...\n",
      "Fetching data for 2024-10-22...\n",
      "Fetching data for 2024-10-23...\n",
      "Fetching data for 2024-10-24...\n",
      "Fetching data for 2024-10-25...\n",
      "Fetching data for 2024-10-26...\n",
      "Fetching data for 2024-10-27...\n",
      "Fetching data for 2024-10-28...\n",
      "Fetching data for 2024-10-29...\n",
      "Fetching data for 2024-10-30...\n",
      "Fetching data for 2024-10-31...\n",
      "Fetching data for 2024-11-01...\n",
      "Fetching data for 2024-11-02...\n",
      "Fetching data for 2024-11-03...\n",
      "Fetching data for 2024-11-04...\n",
      "Fetching data for 2024-11-05...\n",
      "Fetching data for 2024-11-06...\n",
      "Fetching data for 2024-11-07...\n",
      "Fetching data for 2024-11-08...\n",
      "Fetching data for 2024-11-09...\n",
      "Fetching data for 2024-11-10...\n",
      "Fetching data for 2024-11-11...\n",
      "Fetching data for 2024-11-12...\n",
      "Fetching data for 2024-11-13...\n",
      "Fetching data for 2024-11-14...\n",
      "Fetching data for 2024-11-15...\n",
      "Fetching data for 2024-11-16...\n",
      "Fetching data for 2024-11-17...\n",
      "Fetching data for 2024-11-18...\n",
      "Fetching data for 2024-11-19...\n",
      "Fetching data for 2024-11-20...\n",
      "Fetching data for 2024-11-21...\n",
      "Fetching data for 2024-11-22...\n",
      "Fetching data for 2024-11-23...\n",
      "Fetching data for 2024-11-24...\n",
      "Fetching data for 2024-11-25...\n",
      "Fetching data for 2024-11-26...\n",
      "Fetching data for 2024-11-27...\n",
      "Fetching data for 2024-11-28...\n",
      "Fetching data for 2024-11-29...\n",
      "Fetching data for 2024-11-30...\n",
      "Fetching data for 2024-12-01...\n",
      "Fetching data for 2024-12-02...\n",
      "Fetching data for 2024-12-03...\n",
      "Fetching data for 2024-12-04...\n",
      "Fetching data for 2024-12-05...\n",
      "Fetching data for 2024-12-06...\n",
      "Fetching data for 2024-12-07...\n",
      "Fetching data for 2024-12-08...\n",
      "Fetching data for 2024-12-09...\n",
      "Fetching data for 2024-12-10...\n",
      "Fetching data for 2024-12-11...\n",
      "Fetching data for 2024-12-12...\n",
      "Fetching data for 2024-12-13...\n",
      "Fetching data for 2024-12-14...\n",
      "Fetching data for 2024-12-15...\n",
      "Fetching data for 2024-12-16...\n",
      "Fetching data for 2024-12-17...\n",
      "Fetching data for 2024-12-18...\n",
      "Fetching data for 2024-12-19...\n",
      "Fetching data for 2024-12-20...\n",
      "Fetching data for 2024-12-21...\n",
      "Fetching data for 2024-12-22...\n",
      "Fetching data for 2024-12-23...\n",
      "Fetching data for 2024-12-24...\n",
      "Fetching data for 2024-12-25...\n",
      "Fetching data for 2024-12-26...\n",
      "Fetching data for 2024-12-27...\n",
      "Fetching data for 2024-12-28...\n",
      "Fetching data for 2024-12-29...\n",
      "Fetching data for 2024-12-30...\n",
      "Fetching data for 2024-12-31...\n",
      "Fetching data for 2025-01-01...\n",
      "Fetching data for 2025-01-02...\n",
      "Fetching data for 2025-01-03...\n",
      "Fetching data for 2025-01-04...\n",
      "Fetching data for 2025-01-05...\n",
      "Fetching data for 2025-01-06...\n",
      "Fetching data for 2025-01-07...\n",
      "Fetching data for 2025-01-08...\n",
      "Fetching data for 2025-01-09...\n",
      "Fetching data for 2025-01-10...\n",
      "Fetching data for 2025-01-11...\n",
      "Fetching data for 2025-01-12...\n",
      "Fetching data for 2025-01-13...\n",
      "Fetching data for 2025-01-14...\n",
      "Fetching data for 2025-01-15...\n",
      "Fetching data for 2025-01-16...\n",
      "Fetching data for 2025-01-17...\n",
      "Fetching data for 2025-01-18...\n",
      "Fetching data for 2025-01-19...\n",
      "Fetching data for 2025-01-20...\n",
      "Fetching data for 2025-01-21...\n",
      "Fetching data for 2025-01-22...\n",
      "Fetching data for 2025-01-23...\n",
      "Fetching data for 2025-01-24...\n",
      "Fetching data for 2025-01-25...\n",
      "Fetching data for 2025-01-26...\n",
      "Fetching data for 2025-01-27...\n",
      "Fetching data for 2025-01-28...\n",
      "Fetching data for 2025-01-29...\n",
      "Fetching data for 2025-01-30...\n",
      "Fetching data for 2025-01-31...\n",
      "Fetching data for 2025-02-01...\n",
      "Fetching data for 2025-02-02...\n",
      "Fetching data for 2025-02-03...\n",
      "Fetching data for 2025-02-04...\n",
      "Fetching data for 2025-02-05...\n",
      "Fetching data for 2025-02-06...\n",
      "Fetching data for 2025-02-07...\n",
      "Fetching data for 2025-02-08...\n",
      "Fetching data for 2025-02-09...\n",
      "Fetching data for 2025-02-10...\n",
      "Fetching data for 2025-02-11...\n",
      "Fetching data for 2025-02-12...\n",
      "Fetching data for 2025-02-13...\n",
      "Fetching data for 2025-02-14...\n",
      "Fetching data for 2025-02-15...\n",
      "Fetching data for 2025-02-16...\n",
      "Fetching data for 2025-02-17...\n",
      "Fetching data for 2025-02-18...\n",
      "Fetching data for 2025-02-19...\n",
      "Fetching data for 2025-02-20...\n",
      "Fetching data for 2025-02-21...\n",
      "Fetching data for 2025-02-22...\n",
      "Fetching data for 2025-02-23...\n",
      "Fetching data for 2025-02-24...\n",
      "Fetching data for 2025-02-25...\n",
      "Fetching data for 2025-02-26...\n",
      "Fetching data for 2025-02-27...\n",
      "Fetching data for 2025-02-28...\n",
      "Fetching data for 2025-03-01...\n",
      "Fetching data for 2025-03-02...\n",
      "Fetching data for 2025-03-03...\n",
      "Fetching data for 2025-03-04...\n",
      "Fetching data for 2025-03-05...\n",
      "Fetching data for 2025-03-06...\n",
      "Fetching data for 2025-03-07...\n",
      "Fetching data for 2025-03-08...\n",
      "Fetching data for 2025-03-09...\n",
      "Fetching data for 2025-03-10...\n",
      "Fetching data for 2025-03-11...\n",
      "Fetching data for 2025-03-12...\n",
      "Fetching data for 2025-03-13...\n",
      "Fetching data for 2025-03-14...\n",
      "Fetching data for 2025-03-15...\n",
      "Fetching data for 2025-03-16...\n",
      "Fetching data for 2025-03-17...\n",
      "Fetching data for 2025-03-18...\n",
      "Fetching data for 2025-03-19...\n",
      "Fetching data for 2025-03-20...\n",
      "Fetching data for 2025-03-21...\n",
      "Fetching data for 2025-03-22...\n",
      "Fetching data for 2025-03-23...\n",
      "Fetching data for 2025-03-24...\n",
      "Fetching data for 2025-03-25...\n",
      "Fetching data for 2025-03-26...\n",
      "Fetching data for 2025-03-27...\n",
      "Fetching data for 2025-03-28...\n",
      "Fetching data for 2025-03-29...\n",
      "Fetching data for 2025-03-30...\n",
      "Fetching data for 2025-03-31...\n",
      "Fetching data for 2025-04-01...\n",
      "Saved to rainfall_data.csv\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# def get_rainfall_data(date_str):\n",
    "#     url = f\"https://api.data.gov.sg/v1/environment/rainfall?date={date_str}\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch data for {date_str}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     data = response.json()\n",
    "\n",
    "#     if not data.get(\"items\") or not data[\"items\"][0].get(\"readings\"):\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     readings = data[\"items\"][0][\"readings\"]\n",
    "#     stations = data[\"metadata\"][\"stations\"]\n",
    "#     timestamp = data[\"items\"][0][\"timestamp\"]\n",
    "\n",
    "#     # Merge readings with station info\n",
    "#     readings_df = pd.DataFrame(readings)\n",
    "#     stations_df = pd.DataFrame(stations)\n",
    "\n",
    "#     merged_df = readings_df.merge(stations_df, left_on=\"station_id\", right_on=\"id\")\n",
    "\n",
    "#     # Keep only useful columns\n",
    "#     concise_df = merged_df[[\"station_id\", \"name\", \"location\", \"value\"]].copy()\n",
    "#     concise_df[\"latitude\"] = concise_df[\"location\"].apply(lambda x: x[\"latitude\"])\n",
    "#     concise_df[\"longitude\"] = concise_df[\"location\"].apply(lambda x: x[\"longitude\"])\n",
    "#     concise_df[\"timestamp\"] = timestamp\n",
    "#     concise_df.rename(columns={\n",
    "#         \"name\": \"station_name\",\n",
    "#         \"value\": \"rainfall_mm\"\n",
    "#     }, inplace=True)\n",
    "\n",
    "#     return concise_df[[\"station_id\", \"station_name\", \"latitude\", \"longitude\", \"rainfall_mm\", \"timestamp\"]]\n",
    "\n",
    "# # Date range settings\n",
    "# start_date = datetime(2024, 1, 1)\n",
    "# end_date = datetime(2025, 4, 1)\n",
    "\n",
    "# all_data = []\n",
    "# current_date = start_date\n",
    "# while current_date <= end_date:\n",
    "#     date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "#     print(f\"Fetching data for {date_str}...\")\n",
    "#     day_data = get_rainfall_data(date_str)\n",
    "#     if not day_data.empty:\n",
    "#         all_data.append(day_data)\n",
    "#     current_date += timedelta(days=1)\n",
    "\n",
    "# # Combine and save\n",
    "# final_df = pd.concat(all_data, ignore_index=True)\n",
    "# final_df.to_csv(\"rainfall_data.csv\", index=False)\n",
    "# print(\"Saved to rainfall_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic Volume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TrafficFlow JSON Structure:\n",
      "{\n",
      "  \"odata.metadata\": \"http://datamall2.mytransport.sg/ltaodataservice/$metadata#TrafficFlow\",\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"Link\": \"https://dmprod-datasets.s3.ap-southeast-1.amazonaws.com/traffic-flow/data/trafficflow.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEA8aDmFwLXNvdXRoZWFzdC0xIkgwRgIhAMqIgXd2rA%2FBTlWRLVekNDrHVBVrv39ifrHUX6%2Fh9FiEAiEA%2Ffr6pGWFFP7uE0%2FkpMqCE32k%2BKE85g8WyRYkX9N5MwsqywUI6P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzNDA2NDUzODEzMDQiDOFoU1gzxZbz1q34SSqfBWpnf4kROuFYAZ1aVsUAW1E8SMs21x9TIurTwFLnrjF1fIqUnDzs5f6tKgzZSAzM5l87V7qzBI%2FFTWUeavZvduLeAr2iRgJq7ROGHTm3OLBydvPNo%2Fihr2xp5vw7gsS4%2F7DQVArwmQl32999q7IxmTQZ3W8T8vjxaAiBm%2Bbj%2BgSIBBIlTwcMMHa2nXebOji4qK0cjT36ZbimioN1ZFOM%2F23XAD%2BXrKKOlk3eUjPHywfLJJDKCslQflhfnp3CVoZvF7qmhKqd9nmneRi2NklovdMASArbs2pINsNqI8Hb5s0C56g%2FcpAExwpPtlyxhagtTg4L92goKyGJ7LNzTHUJloDFwp0oYpdKJvj2SwZw7XGTThMFMX3lkM4CrFg80SYEqjt7ZXKHAbKTLHPI7MLhlmKPsuQ4SNZSAP6IQygCLlMvCEwzzzQe%2BlH9wvRP%2F8aBd7WUdsksia7fVHFhLn83Xn%2B8XXXRQcj3ZVqedsH5b1%2B6cWZO0eGRRAekaw%2Bu5UHFh2UNLU%2FvDnzJ5pE8y3tgVUpEWvof222qhDspj5oyq9%2FYp54SfMnbpP8FkIr0KimqN%2BglwC2lrlJipEWvtBNt7NLgxanKWJ%2BXhHYpdFnWadUYbqxsWHba4DKW51YRim1x4H40VyOM99UjO1V9eorRd8Uu9CbVNS9tzNW0Uhm0aV%2BPx0LMHs8NYizLCGJ695vYjiOb%2B%2FFLiyh8eb6My%2BkBLWA3Lo%2FuOVe%2BOwn1FbvFv%2B%2B3DP8SblIbQgydve0ToWx6i5wqsIKKe%2FnAiQ%2FvRpFd7CaL%2BEVe2Jamg%2F82nRmxbdw%2BKIQhBGLEl8OoBZBNgt8Lb%2Fe2dqJn4YLRevcBmR4lKfrarq5YiF6vcu94GhU4Vb5ZJ6tUA79lLdGN6C5lR2QBMJKVr6EGOrABq4AVDK10oGqCgyqLLXaUVUEwiNw7fa8rpkMS2JcBfQ9wNUVE%2FpyyxDsJZnhSebIsnLUJcrEVap8ldb%2FQpWsBq%2BIoCjmpA9MGQovBKvtEL0463bJS0YKQkuL5W2c5EYx5MKG7IU1YI6%2F93Avte9o2NJl4LCbP1XKlVLVSckcg2hQP5OvixBYHax%2B4%2FhPSFcAGiP36b%2F8CSMl0KgR42lc2pvYZ1kZoxr7t05U6m90iMT4%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230404T072059Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAU6UAMAS4LBW6NJJO%2F20230404%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Signature=dc7cca358e87eb7686b6e92b354069acd323fabf8427dff081bfbc81f6c0300b\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Rainfall across SG \n",
    "with open(\"TrafficFlow.json\", \"r\") as file:\n",
    "    trafficflow_data = json.load(file)\n",
    "\n",
    "print(\"\\nTrafficFlow JSON Structure:\")\n",
    "print(json.dumps(trafficflow_data, indent=2))  # Pretty print JSON structure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys in JSON: dict_keys(['LastUpdatedDate', 'Value'])\n",
      "{\n",
      "  \"LastUpdatedDate\": \"2025-03-03\",\n",
      "  \"Value\": [\n",
      "    {\n",
      "      \"LinkID\": \"103000122\",\n",
      "      \"Date\": \"03/11/2024\",\n",
      "      \"HourOfDate\": \"7\",\n",
      "      \"Volume\": \"1761\",\n",
      "      \"StartLon\": \"103.6944089\",\n",
      "      \"StartLat\": \"1.350438528\",\n",
      "      \"EndLon\": \"103.6953675\",\n",
      "      \"EndLat\": \"1.351780186\",\n",
      "      \"RoadName\": \"PAN ISLAND EXPRESSWAY\",\n",
      "      \"RoadCat\": \"CAT1\"\n",
      "    },\n",
      "    {\n",
      "      \"LinkID\": \"103000122\",\n",
      "      \"Date\": \"24/11/2024\",\n",
      "      \"HourOfDate\": \"7\",\n",
      "      \"Volume\": \"1798\",\n",
      "      \"StartLon\": \"103.6944089\",\n",
      "      \"StartLat\": \"1.350438528\",\n",
      "      \"EndLon\": \"103.6953675\",\n",
      "      \"EndLat\": \"1.351780186\",\n",
      "      \"RoadName\": \"PAN ISLAND EXPRESSWAY\",\n",
      "      \"RoadCat\": \"CAT1\"\n",
      "    },\n",
      "    {\n",
      "      \"LinkID\": \"103000122\",\n",
      "      \"Date\": \"10/11/2024\",\n",
      "      \"HourOfDate\": \"7\",\n",
      "      \"Volume\": \"1914\",\n",
      "      \"StartLon\": \"103.6944089\",\n",
      "      \"StartLat\": \"1.350438528\",\n",
      "      \"EndLon\": \"103.6953675\",\n",
      "      \"EndLat\": \"1.351780186\",\n",
      "      \"RoadName\": \"PAN ISLAND EXPRESSWAY\",\n",
      "      \"RoadCat\": \"CAT1\"\n",
      "    },\n",
      "    {\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://dmprod-datasets.s3.ap-southeast-1.amazonaws.com/traffic-flow/data/trafficflow.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFEaDmFwLXNvdXRoZWFzdC0xIkgwRgIhAOVWE2UQtuUmOaUDPIlr4wH4HxTbANA9zGeAZBZ%2BbHsIAiEAujIeZuY9KO8%2FmVMXE1ULvBoOFz%2FrTYcJqlnxjvB0LLQqywUIuv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzNDA2NDUzODEzMDQiDDjXF%2BbKby54bAl5NyqfBWBIlYMVspNgnhgPOh41qHuBk%2FNNFG65h5RYVKxZUt%2FOHWWvjlyhUV6VPq5%2BgpqZxM6Yl%2BWSHOeccOp6V9WUlENuGP0DkSu2RZ7qrdWmeO5IrVvOZGNlsEZU6XjkK%2F3eDtlDGVx4ZoSG1p73JlxZw8BlfZ7m29zHjDMQKGF%2FH6zhjp8dtj4Ywi7Nr0KtzlM9z4SMzw%2F0l6kzwyhyS2LOhi44CTYve%2FPci%2F2U%2FZkuL%2FHUwGUcmnPxCG3RLl02OHk9gZT4%2Fqj46Uc6vsNP%2BZykVLh336mFfRtQRa08xfITvAWPikLtQ8eJDuHnhJznlEhsmk%2Bw04gOhXjflW%2BgnzhcGLWsD9qswzCIeNzDRc24xdOAtu88EsqKuCYaBvQO1ywO%2BDtYTe8JE6RZhxQegRLORKBRURZ12uiQ4fXXV5R0h3X4fAOVWCnXARMKa1%2BfoUkGVzejdvl4JKPe13UEbePHq7vm5IC1TKPLIjphZ7Sj0itSvFtKkgHPBD5XYp5XCxuldZALKO1eQCLKoZ7D2xL58hulPQ061x6kBIkAmByMMiXjXDPBKJZLPV3Y%2B0m1aswfPzKQyNddHg3gAO2DfL5fsKan01pr19Bq76TQkGu68Eakf8vVzNql0vJsgXqetrp8NuAvlMFN%2BfDqO4j1rZVNWPlVPpLPIla98A5n5sDbphWm5L7FSvnblPWdwbmmP842vEOdXKTL2pyshD0eUzSL6mYBB%2FyjqFT2vSqxGO35cA8VAzuZ9SVy60dn%2BpzRz8qlyQVD4X78O1puqBEjlzuqMCq%2BGK%2B%2BoFEcBaP9xjjvo9deYwds%2Byz9mqt6PF5cecegdbzyFa90L3vwZosAlEqdjrohOUF2fWAot1KzGCuQ%2FDPDMwLttRrD5PQlsQ9ODKSxMJHgrr8GOrABZrFFUW9ma0FVGvfVPpoUEXwwTKCdqHWB75Q0eHr5cmjDiJs7qX2T1A6cTTRXNWUho9PgXZc55xon4l%2FqX3GuIrvk%2BS9XaLTHFLytzsNhJpGRW99PTw9aus0F8r1t2Ez2uVsrXmlHOScA9rDH8OiF625EdjoWvXFUjrfuhv0Iz6zz1fUDmNhtt1qEu4TlCRjcUFKUNNZFP2YDSGYl%2FR9LbJQruTgRKTBKSsEjqIskw9k%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250401T104037Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAU6UAMAS4MKFA4KUX%2F20250401%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Signature=690a2f1734ba4291b970439cbeecfbc7575bdf7904d5c0d7c1c84d76503c660e\"\n",
    "response = requests.get(url)\n",
    "\n",
    "try:\n",
    "    traffic_json = response.json()\n",
    "    print(\"Top-level keys in JSON:\", traffic_json.keys())\n",
    "    print(json.dumps(traffic_json, indent=2)[:1000])  # print first part nicely\n",
    "except Exception as e:\n",
    "    print(\"Failed to decode JSON:\", e)\n",
    "    print(\"Raw response:\", response.text[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Traffic flow data saved to traffic_flow_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your valid pre-signed S3 URL (updated if needed)\n",
    "url = \"https://dmprod-datasets.s3.ap-southeast-1.amazonaws.com/traffic-flow/data/trafficflow.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFEaDmFwLXNvdXRoZWFzdC0xIkgwRgIhAOVWE2UQtuUmOaUDPIlr4wH4HxTbANA9zGeAZBZ%2BbHsIAiEAujIeZuY9KO8%2FmVMXE1ULvBoOFz%2FrTYcJqlnxjvB0LLQqywUIuv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzNDA2NDUzODEzMDQiDDjXF%2BbKby54bAl5NyqfBWBIlYMVspNgnhgPOh41qHuBk%2FNNFG65h5RYVKxZUt%2FOHWWvjlyhUV6VPq5%2BgpqZxM6Yl%2BWSHOeccOp6V9WUlENuGP0DkSu2RZ7qrdWmeO5IrVvOZGNlsEZU6XjkK%2F3eDtlDGVx4ZoSG1p73JlxZw8BlfZ7m29zHjDMQKGF%2FH6zhjp8dtj4Ywi7Nr0KtzlM9z4SMzw%2F0l6kzwyhyS2LOhi44CTYve%2FPci%2F2U%2FZkuL%2FHUwGUcmnPxCG3RLl02OHk9gZT4%2Fqj46Uc6vsNP%2BZykVLh336mFfRtQRa08xfITvAWPikLtQ8eJDuHnhJznlEhsmk%2Bw04gOhXjflW%2BgnzhcGLWsD9qswzCIeNzDRc24xdOAtu88EsqKuCYaBvQO1ywO%2BDtYTe8JE6RZhxQegRLORKBRURZ12uiQ4fXXV5R0h3X4fAOVWCnXARMKa1%2BfoUkGVzejdvl4JKPe13UEbePHq7vm5IC1TKPLIjphZ7Sj0itSvFtKkgHPBD5XYp5XCxuldZALKO1eQCLKoZ7D2xL58hulPQ061x6kBIkAmByMMiXjXDPBKJZLPV3Y%2B0m1aswfPzKQyNddHg3gAO2DfL5fsKan01pr19Bq76TQkGu68Eakf8vVzNql0vJsgXqetrp8NuAvlMFN%2BfDqO4j1rZVNWPlVPpLPIla98A5n5sDbphWm5L7FSvnblPWdwbmmP842vEOdXKTL2pyshD0eUzSL6mYBB%2FyjqFT2vSqxGO35cA8VAzuZ9SVy60dn%2BpzRz8qlyQVD4X78O1puqBEjlzuqMCq%2BGK%2B%2BoFEcBaP9xjjvo9deYwds%2Byz9mqt6PF5cecegdbzyFa90L3vwZosAlEqdjrohOUF2fWAot1KzGCuQ%2FDPDMwLttRrD5PQlsQ9ODKSxMJHgrr8GOrABZrFFUW9ma0FVGvfVPpoUEXwwTKCdqHWB75Q0eHr5cmjDiJs7qX2T1A6cTTRXNWUho9PgXZc55xon4l%2FqX3GuIrvk%2BS9XaLTHFLytzsNhJpGRW99PTw9aus0F8r1t2Ez2uVsrXmlHOScA9rDH8OiF625EdjoWvXFUjrfuhv0Iz6zz1fUDmNhtt1qEu4TlCRjcUFKUNNZFP2YDSGYl%2FR9LbJQruTgRKTBKSsEjqIskw9k%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250401T104037Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAU6UAMAS4MKFA4KUX%2F20250401%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Signature=690a2f1734ba4291b970439cbeecfbc7575bdf7904d5c0d7c1c84d76503c660e\"\n",
    "\n",
    "# Step 1: Get the data\n",
    "response = requests.get(url)\n",
    "traffic_json = response.json()\n",
    "\n",
    "# Step 2: Extract and normalize 'Value'\n",
    "traffic_df = pd.json_normalize(traffic_json[\"Value\"])\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "traffic_df.to_csv(\"traffic_flow_data.csv\", index=False)\n",
    "\n",
    "print(\"✅ Traffic flow data saved to traffic_flow_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Road Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "G = ox.graph_from_place(\"Singapore\", network_type='drive')\n",
    "ox.save_graph_geopackage(G, filepath=\"singapore_road_network.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          u           v  key  \\\n",
      "0  25451929  6749812859    0   \n",
      "1  25451929  2706536622    0   \n",
      "2  25451929  6064305393    0   \n",
      "3  25455287  1637003462    0   \n",
      "4  25455287  2521018789    0   \n",
      "\n",
      "                                               osmid        highway  \\\n",
      "0  [49961799, 718881456, 741164883, 754786742, 17...       motorway   \n",
      "1        [46337834, 140562819, 244833526, 738993583]  motorway_link   \n",
      "2                                          627326844       motorway   \n",
      "3                                          150829205  motorway_link   \n",
      "4                   [633215386, 74607482, 635109319]       motorway   \n",
      "\n",
      "             lanes maxspeed                           name  oneway  ref  \\\n",
      "0                5       70             East Coast Parkway    True  ECP   \n",
      "1  ['2', '3', '1']       50                                   True        \n",
      "2                3       80             East Coast Parkway    True  ECP   \n",
      "3                1       50  Kallang Paya Lebar Expressway    True        \n",
      "4       ['2', '3']       90             East Coast Parkway    True  ECP   \n",
      "\n",
      "  reversed      length bridge        from          to tunnel junction access  \\\n",
      "0    False  765.027747    yes    25451929  6749812859                          \n",
      "1    False  842.231447         2706536622    25451929    yes                   \n",
      "2    False  402.941950         6064305393    25451929                          \n",
      "3    False  629.055082           25455287  1637003462                          \n",
      "4    False  652.575579           25455287  2521018789                          \n",
      "\n",
      "  width                                           geometry  \n",
      "0        LINESTRING (103.87254 1.29523, 103.87103 1.295...  \n",
      "1        LINESTRING (103.87717 1.29738, 103.87648 1.295...  \n",
      "2        LINESTRING (103.87617 1.29534, 103.87556 1.295...  \n",
      "3        LINESTRING (103.874 1.29544, 103.87413 1.2955,...  \n",
      "4        LINESTRING (103.874 1.29544, 103.87438 1.29544...  \n",
      "      osmid         y           x  street_count            highway  ref  \\\n",
      "0  25451929  1.295232  103.872544             3                           \n",
      "1  25455287  1.295445  103.874004             3  motorway_junction  14A   \n",
      "2  26777521  1.303979  103.823571             3                           \n",
      "3  26778779  1.304654  103.851530             3                           \n",
      "4  26778790  1.301532  103.850025             4    traffic_signals        \n",
      "\n",
      "  junction                   geometry  \n",
      "0           POINT (103.87254 1.29523)  \n",
      "1             POINT (103.874 1.29544)  \n",
      "2           POINT (103.82357 1.30398)  \n",
      "3           POINT (103.85153 1.30465)  \n",
      "4           POINT (103.85002 1.30153)  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the road segments\n",
    "edges = gpd.read_file(\"singapore_road_network.gpkg\", layer=\"edges\")\n",
    "\n",
    "# Load the intersection points\n",
    "nodes = gpd.read_file(\"singapore_road_network.gpkg\", layer=\"nodes\")\n",
    "\n",
    "print(edges.head())\n",
    "print(nodes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "\n",
    "edges = gpd.read_file(\"data/singapore_road_network.gpkg\", layer=\"edges\")\n",
    "G = nx.DiGraph()\n",
    "for idx, row in edges.iterrows():\n",
    "    if isinstance(row.geometry, LineString):\n",
    "        # Use first and last coordinate as nodes\n",
    "        coords = list(row.geometry.coords)\n",
    "        u = coords[0]  \n",
    "        v = coords[-1] \n",
    "        G.add_edge(u, v, **row.drop('geometry').to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic columns: ['LinkID', 'Date', 'HourOfDate', 'Volume', 'StartLon', 'StartLat', 'EndLon', 'EndLat', 'RoadName', 'RoadCat']\n",
      "Edges columns: ['u', 'v', 'key', 'osmid', 'highway', 'lanes', 'maxspeed', 'name', 'oneway', 'ref', 'reversed', 'length', 'bridge', 'from', 'to', 'tunnel', 'junction', 'access', 'width', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "print(\"Traffic columns:\", traffic.columns.tolist())\n",
    "print(\"Edges columns:\", edges.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.read_csv(\"data/traffic_flow_data.csv\")\n",
    "traffic_agg = traffic.groupby(\"RoadName\")[[\"Volume\"]].mean().reset_index()\n",
    "edges = edges.merge(traffic_agg, left_on=\"name\", right_on=\"RoadName\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainfall original columns: ['station_id', 'station_name', 'latitude', 'longitude', 'rainfall_mm', 'timestamp', 'geometry']\n",
      "Rainfall joined columns: ['station_id', 'station_name', 'latitude', 'longitude', 'rainfall_mm_left', 'timestamp', 'geometry', 'index_right', 'u', 'v', 'key', 'osmid', 'highway', 'lanes', 'maxspeed', 'name', 'oneway', 'ref', 'reversed', 'length', 'bridge', 'from', 'to', 'tunnel', 'junction', 'access', 'width', 'RoadName', 'Volume', 'rainfall_mm_right', 'dist_to_road']\n"
     ]
    }
   ],
   "source": [
    "print(\"Rainfall original columns:\", rainfall.columns.tolist())\n",
    "print(\"Rainfall joined columns:\", rainfall_joined.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall = pd.read_csv(\"data/rainfall_data.csv\")\n",
    "\n",
    "# Convert to GeoDataFrame using EPSG:4326 (WGS84)\n",
    "rainfall[\"geometry\"] = rainfall.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
    "rainfall_gdf = gpd.GeoDataFrame(rainfall, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Project both rainfall and edges to projected CRS (SVY21 for Singapore: EPSG 3414)\n",
    "rainfall_gdf = rainfall_gdf.to_crs(epsg=3414)\n",
    "edges = edges.to_crs(epsg=3414)\n",
    "\n",
    "# Spatial join: assign rainfall stations to nearest road segment\n",
    "rainfall_joined = gpd.sjoin_nearest(\n",
    "    rainfall_gdf,\n",
    "    edges,\n",
    "    how=\"left\",\n",
    "    distance_col=\"dist_to_road\"\n",
    ")\n",
    "\n",
    "# Group by road name ('name' from edges) and average the rainfall readings\n",
    "rainfall_agg = rainfall_joined.groupby(\"name\")[[\"rainfall_mm_left\"]].mean().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "rainfall_agg.rename(columns={\"rainfall_mm_left\": \"rainfall_mm\"}, inplace=True)\n",
    "\n",
    "# Merge rainfall data into road edge GeoDataFrame\n",
    "edges = edges.merge(rainfall_agg, on=\"name\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DataSeries', '2025Feb', '2025Jan', '2024Dec', '2024Nov', '2024Oct', '2024Sep', '2024Aug', '2024Jul', '2024Jun', '2024May', '2024Apr', '2024Mar', '2024Feb', '2024Jan', '2023Dec', '2023Nov', '2023Oct', '2023Sep', '2023Aug', '2023Jul', '2023Jun', '2023May', '2023Apr', '2023Mar', '2023Feb', '2023Jan', '2022Dec', '2022Nov', '2022Oct', '2022Sep', '2022Aug', '2022Jul', '2022Jun', '2022May', '2022Apr', '2022Mar', '2022Feb', '2022Jan', '2021Dec', '2021Nov', '2021Oct', '2021Sep', '2021Aug', '2021Jul', '2021Jun', '2021May', '2021Apr', '2021Mar', '2021Feb', '2021Jan', '2020Dec', '2020Nov', '2020Oct', '2020Sep', '2020Aug', '2020Jul', '2020Jun', '2020May', '2020Apr', '2020Mar', '2020Feb', '2020Jan', '2019Dec', '2019Nov', '2019Oct', '2019Sep', '2019Aug', '2019Jul', '2019Jun', '2019May', '2019Apr', '2019Mar', '2019Feb', '2019Jan', '2018Dec', '2018Nov', '2018Oct', '2018Sep', '2018Aug', '2018Jul', '2018Jun', '2018May', '2018Apr', '2018Mar', '2018Feb', '2018Jan', '2017Dec', '2017Nov', '2017Oct', '2017Sep', '2017Aug', '2017Jul', '2017Jun', '2017May', '2017Apr', '2017Mar', '2017Feb', '2017Jan', '2016Dec', '2016Nov', '2016Oct', '2016Sep', '2016Aug', '2016Jul', '2016Jun', '2016May', '2016Apr', '2016Mar', '2016Feb', '2016Jan', '2015Dec', '2015Nov', '2015Oct', '2015Sep', '2015Aug', '2015Jul', '2015Jun', '2015May', '2015Apr', '2015Mar', '2015Feb', '2015Jan', '2014Dec', '2014Nov', '2014Oct', '2014Sep', '2014Aug', '2014Jul', '2014Jun', '2014May', '2014Apr', '2014Mar', '2014Feb', '2014Jan', '2013Dec', '2013Nov', '2013Oct', '2013Sep', '2013Aug', '2013Jul', '2013Jun', '2013May', '2013Apr', '2013Mar', '2013Feb', '2013Jan', '2012Dec', '2012Nov', '2012Oct', '2012Sep', '2012Aug', '2012Jul', '2012Jun', '2012May', '2012Apr', '2012Mar', '2012Feb', '2012Jan', '2011Dec', '2011Nov', '2011Oct', '2011Sep', '2011Aug', '2011Jul', '2011Jun', '2011May', '2011Apr', '2011Mar', '2011Feb', '2011Jan', '2010Dec', '2010Nov', '2010Oct', '2010Sep', '2010Aug', '2010Jul', '2010Jun', '2010May', '2010Apr', '2010Mar', '2010Feb', '2010Jan', '2009Dec', '2009Nov', '2009Oct', '2009Sep', '2009Aug', '2009Jul', '2009Jun', '2009May', '2009Apr', '2009Mar', '2009Feb', '2009Jan']\n"
     ]
    }
   ],
   "source": [
    "print(accidents.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
